#+title: Chapter 10 - Modern RNNs

* LSTM
- As discussed in the previous chapters, We can solve the exploding gradients problem with gradient clipping, but we still suffer
  from vanishing gradients problem. LSTMs (Long Short-Term Memory networks) were designed specifically to address this problem.

** The Memory Cell
- LSTMs are essentially RNNs with a special "memory cell" that replaces the ordinary recurrent node in standard RNNs.
- This memory cell is the heart of the LSTM and provides a mechanism for selectively remembering or forgetting information over time.
- Each LSTM contains both long-term memory (weights) and short-term memory (activations), as well as a special medium-term memory.
- The memory cell has an internal state and a number of "gates" that control the flow of information into and out of the cell.
  - *Input Gate:* How much of the new input should affect the internal state.
  - *Forget Gate:* Whether the current value of the memory should be flushed (set to zero).
  - *Output Gate:* Whether the internal state of the neuron is allowed to affect the cell's output.

- The purpose of the "gates" is to selectively modify the hidden state and thus enable longer term learning,
  and help with the vanishing gradient problem.

** Input Gate, Forget Gate and Output Gate
- The data feeding into the LSTM gates are the input at the current time step and the hidden state of the previous time step.
   
  [[./images/LSTM1.png]] 

- It = σ(Xt Wxi + Ht-1 Whi + bi)
- Ft = σ(Xt Wxf + Ht-1 Whf + bf)
- Ot = σ(Xt Wxo + Ht-1 Who + bo)

- It, Ft, Ot ∈ R^{n x h}: The values of the input, forget, and output gates at time step t. The numbers are between 0 and 1 due to the sigmoid function.
- σ: The sigmoid activation function. This squashes the values between 0 and 1, representing the degree to which the gate is "open" or "closed."
- Xt ∈ R^{n x d}: The input at time step t. (Batch size n, input features d)
- Ht-1 ∈ R^{n x h}: The hidden state from the previous time step. (Batch size n, hidden units h)
- Wxi, Wxf, Wxo ∈ R^(d x h): Weight matrices connecting the input Xt to the input, forget, and output gates, respectively.
- Whi, Whf, Who ∈ R^(h x h): Weight matrices connecting the previous hidden state Ht-1 to the input, forget, and output gates, respectively.
- bi, bf, bo ∈ R^(1 x h): Bias vectors for the input, forget, and output gates, respectively.

** Input Node
- Ct = tanh(Xt Wxc + Ht-1 Whc + bc)
