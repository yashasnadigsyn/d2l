#+title: Chapter 8 - Modern CNN
* AlexNet
- AlexNet is an 8-layer convolutional neural network (CNN).

  [[./images/modern_CNN_1.png]]

- Left - LeNet, Right - AlexNet
- Used MaxPool instead of AvgPool.
- Used dropout techniques.
- Used ReLU instead of Sigmoid.

* VGG
- AlexNet: Individual convolutional layers with varying kernel sizes.
  - VGG: Introduced the concept of "VGG blocks" - a sequence of:
    1. Multiple convolutional layers with same small (3x3) kernel and padding = 1 (to maintain resolution).
    2. ReLU activation function.
    3. Max-pooling layer (2x2 with stride 2) for downsampling (reducing resolution).

[[./images/modern_CNN_2.png]]

- Architecture:
  - Deeper than AlexNet (e.g., VGG-11 has 11 layers: 8 convolutional, 3 fully connected).
    - Uses only 3x3 convolutional kernels throughout the network.
    - Convolutional layers are grouped into VGG blocks.
    - Each VGG block followed by max-pooling (2x2, stride 2) that halves the spatial dimensions.
    - Number of channels doubles after each block (e.g., 64, 128, 256, 512).
    - The structure is modular. Can create deeper or shallower VGG networks by varying number of blocks and number of convolutions inside each block.
* Network in Network
- *Parameter Explosion*
  - Fully connected layers in networks like AlexNet and VGG have a huge number of parameters.
  - Example: VGG-11's fully connected layers require ~400MB of memory.
  - Issues:
    - Memory intensive: Hard to deploy on devices with limited memory (e.g., mobile phones).
    - Computationally expensive: Slows down training and inference.

- *Loss of Spatial Information*
  - Using fully connected layers early in the network discards spatial relationships learned by convolutional layers.
  - Analogy: Shuffling the pieces of a jigsaw puzzle destroys the image.

- NiN's Solution: 1x1 Convolutions and Global Average Pooling
  - *1x1 Convolutions*
    - Applied after each regular convolution.
    - Act as a fully connected layer *at each pixel location*.
    - Introduce non-linearity across channels without affecting spatial structure.
    - Analogy: A mini neural network at each pixel, learning relationships between feature maps.
    - Benefits:
      - Adds non-linearity.
      - Increases representational power.
      - Can reduce the number of channels (and parameters).

  - *Global Average Pooling*
    - Replaces the final fully connected layers.
    - Computes the average of each feature map across all spatial locations.
    - Produces a single value per feature map.
    - Analogy: Summarizing a document by averaging the sentiment of each paragraph.
    - Benefits:
      - Drastically reduces parameter count (no large weight matrices).
      - Adds robustness to spatial translations.

[[./images/modern_CNN_3.png]]

- NiN Blocks
  - *Definition*
    1. A convolutional layer (kernel size > 1x1, e.g., 3x3, 5x5, 11x11).
    2. Two 1x1 convolutional layers (per-pixel "fully connected" layers).
    3. ReLU activations after each convolutional layer.
* Multi-Branch Networks (GoogLeNet)
- GoogLeNet is a multi-branch CNN architecture.
- It popularized the pattern of stem, body, and head in CNN design.
  - Stem: The initial layers that process the raw input (e.g., the first few convolutional layers). Think of it as the part of the network that "ingests" the image and does initial processing.
  - Body: The main part of the network, consisting of repeated blocks (in this case, Inception blocks) that extract increasingly complex features.
  - Head: The final layers that take the features extracted by the body and make a prediction (e.g., classification, object detection).
- Instead of trying to figure out the best size for convolution kernels (e.g., 3x3, 5x5), GoogLeNet used multiple kernel sizes in parallel and combined their outputs. This allowed it to capture features at different scales effectively.

** Inception Block

[[./images/modern_CNN_4.png]]

- Four Parallel Branches: The Inception block has four paths that process the input simultaneously.

** GoogLeNet Architecture

[[./images/modern_CNN_5.png]]

- Overall Structure: GoogLeNet is built by stacking multiple Inception blocks, with max-pooling layers in between to reduce the spatial dimensions.
- Stem: Similar to earlier networks like AlexNet, it starts with a few convolutional layers to process the raw image.
- Body: The core of the network, made up of nine Inception blocks organized into three groups. The number of channels and the ratios between branches in the Inception blocks are carefully chosen.
- Head: A global average pooling layer reduces the spatial dimensions to 1x1, followed by a fully connected layer for classification.
- Dimensionality Reduction: The max-pooling layers gradually reduce the height and width of the feature maps, while the number of channels increases. This is a common pattern in CNNs: trading spatial resolution for richer feature representations.

* Batch Normalization
** The Problem: Training Deep Networks is hard
- *Vanishing/Exploding Gradients:* In very deep networks, gradients can become extremely small or large as they are backpropagated through many layers. This makes it hard for the optimization algorithm to update the weights effectively.
- *Internal Covariate Shift:* The distribution of the activations (outputs of layers) can change significantly during training as the weights of earlier layers are updated. This means that each layer is constantly having to adapt to a new input distribution, slowing down learning.
- *Sensitivity to Initialization:* Deep networks can be very sensitive to how the weights are initialized. Poor initialization can lead to slow convergence or getting stuck in bad local minima.
- *Overfitting:* Deeper networks have more parameters, making them more prone to overfitting the training data and performing poorly on unseen data.
** The Solution
*** Part 1: Standardization (Preprocessing)
- Transforms data to have zero mean and unit variance.
- *How it's done (for each feature):*
  - Calculate the mean (average) of the feature across all data points.
  - Calculate the standard deviation (a measure of how spread out the data is) of the feature.
  - For each data point, subtract the mean and divide by the standard deviation.
- *Benefits:*
  - *Improved Numerical Stability:* Keeps values in a reasonable range, preventing numerical issues during computation.
  - *Faster Convergence:* Optimizers work better when features are on a similar scale.
  - *Implicit Regularization:* Standardization can act as a form of regularization, reducing overfitting in some cases.
*** Part 2: Batch Normalization (Training)
- Batch Normalization extends the idea of standardization to the internal layers of a deep network. Instead of just standardizing the input data, it standardizes the activations of each layer during training.
- *How it works (for each layer, during each training iteration):*
  - *Calculate Batch Statistics:*
    - Calculate the mean (μ_{B}) and variance (σ_{B}) of the activations within the current minibatch (a small subset of the training data).
  - *Normalize:*
    - Subtract the batch mean and divide by the batch standard deviation (plus a small constant ε for numerical stability). This centers the activations around 0 and scales them to have unit variance.
      - x_normalized = (x - μ_{B}) / (σ_{B }^{}+ ε)
  - *Scale and Shift:*
    - Multiply the normalized activations by a learnable scale parameter (γ) and add a learnable shift parameter (β). This allows the network to learn the optimal mean and variance for each layer, rather than always forcing them to be 0 and 1. This step restores the representation power of the network.
      - y = γ * x_normalized + β
- *Why it's called "Batch" Normalization:* The statistics (mean and variance) are calculated over a batch of data, not the entire dataset.
- Equation: BN(x) = γ * (x - μ_{B}) / (σ_{B} + ε) + β
- *Why it works*
  - *Reduces Internal Covariate Shift:* By normalizing the activations, batch normalization stabilizes the distribution of inputs to each layer, making training faster and more stable.
  - *Allows Higher Learning Rates:* Batch normalization makes the optimization landscape smoother, allowing you to use larger learning rates without causing instability.
  - *Acts as Regularization:* The noise introduced by using batch statistics acts as a form of regularization, reducing overfitting. This is because each example within a batch is normalized based on the other examples in the batch, adding a small amount of variation.
  - *Makes Networks Less Sensitive to Initialization:* Batch normalization reduces the dependence on careful weight initialization.
** Batch Normalization During Inference
- *Difference from Training:* During inference, we don't have a batch of data to calculate statistics from. Instead, we use moving averages of the mean and variance that were computed during training.
- *Moving Averages:* During training, we keep track of a running average of the batch means and variances. These moving averages are then used to normalize activations during inference. This makes the inference deterministic (the same input always gives the same output).
** Batch Normalization Layers
*** Batch Normalization
- Batch normalization is a technique used to improve the training of deep neural networks. It normalizes the inputs of a layer by adjusting and scaling the activations to have a mean of 0 and a variance of 1. This helps in stabilizing and speeding up the training process.
- Why is it needed?
  - During training, the distribution of inputs to each layer can change as the weights of the previous layers are updated. This phenomenon is called internal covariate shift, and it can slow down training because the network has to constantly adapt to the new distributions. Batch normalization reduces this shift by normalizing the inputs.
**** Batch Normalization in Fully Connected Layer vs Convolutional Layer
- In fully connected layers, batch normalization is typically applied after the affine transformation (i.e., the linear transformation Wx+bWx+b) but before the nonlinear activation function (e.g., ReLU, sigmoid).
- h=ϕ(BN(Wx+b))
- In convolutional layers, batch normalization is applied after the convolution operation but before the activation function. The key difference from fully connected layers is that normalization is applied per channel across all spatial locations (height and width of the feature map).
- How it works:
  - Imagine you have a convolutional layer with 64 output channels, and the output feature maps have a size of 28×28. For each of the 64 channels, batch normalization computes the mean and variance across all 28×28 spatial locations and all examples in the minibatch. It then normalizes the activations for each channel independently.
*** Layer Normalization
- Layer normalization is an alternative to batch normalization. Instead of normalizing across the batch dimension, it normalizes across the features of a single example.
- LN(x)= (x-μ)/sigma
- Key Differences from Batch Normalization:
  - Batch Normalization: Normalizes across the batch dimension (i.e., over multiple examples).
  - Layer Normalization: Normalizes across the feature dimension (i.e., within a single example).
- See [[./batch_vs_layer_example.org]] for example

** Discussion on Batch Normalization
*** Intuition Behind Batch Normalization
- Makes the optimization landscape smoother.
- Stabilizes activations during training
*** Critiques of "Internal Covariate Shift"
- Misnomer: Not the same as covariate shift.
- Lacks rigorous theoretical foundation.
*** Alternative Explanations
- Smoother optimization landscape.
- Regularization through noise.
- Rescaling of activations.
*** Practical Aspects
- Stabilizes intermediate outputs.
- Applied differently in fully connected vs. convolutional layers.
- Different behaviors in training vs. prediction mode.
- Improves convergence and acts as a regularizer.
- Robustness considerations (e.g., removing batch normalization).
*** Broader Implications
- Highlights the gap between intuitions and rigorous explanations.
- Batch normalization is widely used despite incomplete understanding.
*** Key Takeaways
- Batch normalization is empirically effective but theoretically unclear.
- Practitioners should separate guiding intuitions from established facts.
* ResNet
** Function Classes
- Let F represent the set of functions a neural network can learn given it's architecture and hyperparameters. The goal is to find the best function f∗_{F}​ within F that approximates the "truth" function f∗ (the ideal function we want to learn).
- If F is too small, the network might not be able to learn f*. If F is too large, the network might overfit or learn suboptimal functions.
- *Nested Function Classes:* If F1⊆F2⊆⋯⊆F6​, then increasing the size of the function class (e.g., adding more layers) guarantees that the network becomes more expressive. This is because the larger function class includes all the smaller ones.
- *Example:* Imagine we are trying to fit a curve to data points. If a function class only includes linear functions (F1​), We might not fit the data well. If we expand the function class to include quadratic functions (F2), we can fit the data better. If F2 includes all linear functions, then moving from F1​ to F2​ is guaranteed to improve the fit.

  [[./images/modern_CNN_6.png]]

- As illustrated by Fig, for non-nested function classes, a larger function class does not always move closer to the “truth” function
  f*. For instance, on the left of Fig, though F3 is closer to f* than F1, F6 moves away and there is no guarantee that further increasing the complexity can reduce the distance from f*. With nested function classes where F1 ⊆ · · · ⊆ F6 on the right of Fig, we can avoid the aforementioned issue from the non-nested function classes.
** ResNet
- ResNet was introduced to address the problem of degradation in very deep networks: as networks get deeper, accuracy saturates and then degrades. This is counterintuitive because deeper networks should be more expressive.
- ResNet introduces residual blocks, which make it easier for the network to learn the identity function f(x)=xf(x)=x. This is done by learning the residual mapping g(x)=f(x)−xg(x)=f(x)−x instead of directly learning f(x)f(x). If the identity mapping is optimal, the network only needs to push g(x)g(x) to zero, which is easier than learning f(x)f(x) from scratch.
- *Residual Connection:* A shortcut connection (or skip connection) is added to bypass one or more layers. This allows the input to flow directly to the output, making it easier to train very deep networks.
- *Example:* Imagine you're stacking layers to approximate a function. If the optimal function is just the identity (i.e., the output should be the same as the input), a regular network has to learn this explicitly. In a ResNet, the network only needs to learn the difference (residual) between the input and output, which is often close to zero. This makes training easier.

  [[./images/modern_CNN_7.png]]

** Residual Block Architecture

[[./images/modern_CNN_8.png]]

- A residual block consists of:
  - Two 3x3 convolutional layers with batch normalization and ReLU activation.
  - A shortcut connection that adds the input directly to the output of the convolutional layers.
  - Optionally, a 1x1 convolutional layer to adjust the number of channels or spatial dimensions if needed.
** ResNet Model Architecture
- The ResNet model consists of:
  * An initial 7x7 convolutional layer with 64 output channels and a stride of 2, followed by a 3x3 max-pooling layer with a stride of 2.
  * Four modules made up of residual blocks. Each module has:
    - A doubling of the number of channels.
    - A halving of the spatial dimensions (height and width) using a stride of 2 in the first residual block.
  * A global average pooling layer and a fully connected layer for classification.
- ResNet Variants:
  - ResNet-18: 18 layers (including convolutional and fully connected layers).
  - ResNet-152: 152 layers (deeper version with more residual blocks).

[[./images/modern_CNN_9.png]]
* ResNext

[[./images/modern_CNN_10.png]]

** The Problem with ResNet
- In ResNet, the trade-off between non-linearity and dimensionality within a block is a key challenge:
  - *Non-linearity:* Adding more layers increases the network's ability to model complex functions.
  - *Dimensionality:* Increasing the width (number of channels) of the convolutional layers allows the network to capture more features.
- However, there is a problem with increasing the number of channels. For example, if a layer takes ci input channels and produces co output channels, the computational cost is proportional to O(ci⋅co). This makes it expensive to simply increase the width of the network.
** Grouped Convolutions
- The key innovation in ResNeXt is the use of grouped convolutions:
  - A standard convolution takes ci​ input channels and produces co​ output channels, with a computational cost of O(ci⋅co).
  - In a grouped convolution, the input channels are divided into g groups, and each group is processed independently. The computational cost is reduced to O(g⋅(ci/g)⋅(co/g))=O(ci​⋅co​/g), making it g times faster.
  - The number of parameters is also reduced by a factor of g, as each group uses smaller matrices.
** ResNeXt Block Design
- The ResNeXt block is designed to address the limitations of grouped convolutions:
  - *1x1 Convolution:* Reduces the number of input channels to a smaller bottleneck size b. This reduces computational cost.
  - *Grouped 3x3 Convolution:* Processes the bottleneck channels in g independent groups.
  - *1x1 Convolution:* Expands the channels back to the original size.
  - *Shortcut Connection:* Adds the input to the output, similar to ResNet.
* Designing Convolution Network Architectures
** Overview
- Modern network design for computer vision has largely relied on human intuition and creativity.
- Key architectures include AlexNet, VGG, NiN, GoogLeNet, ResNets, ResNeXt, and SENets.
- Neural Architecture Search (NAS) has also been used, but it is computationally expensive.
- A new approach by Radosavovic et al. (2020) focuses on designing network design spaces, leading to RegNets.

** Key Architectures
*** AlexNet
- First to beat conventional models on ImageNet.
- Popularized deep networks with stacked convolutional layers.

*** VGG Networks
- Popularized 3x3 convolutions.
- Uses a deep stack of 3x3 convolutions.

*** NiN (Network in Network)
- Introduced 1x1 convolutions for local nonlinearities.
- Aggregates information across all locations at the network head.
*** GoogLeNet
- Introduced Inception blocks with multiple branches of different convolution widths.
- Combines advantages of VGG and NiN.

__license-mozilla-bp ResNets
- Introduced identity mapping (f(x) = x) to allow for very deep networks.
- Still popular almost a decade later.

*** ResNeXt
- Added grouped convolutions for better parameter and computation trade-offs.

*** SENets (Squeeze-and-Excitation Networks)
- Introduced per-channel global attention for efficient information transfer.

** Neural Architecture Search (NAS)
- Uses search strategies (e.g., brute-force, genetic algorithms, reinforcement learning) to find optimal architectures.
- EfficientNets are a notable outcome of NAS.

** RegNets and Design Spaces
- Radosavovic et al. (2020) proposed a strategy to design network design spaces.
- Focuses on optimizing distributions of networks rather than single networks.
- Leads to RegNetX and RegNetY, along with guiding principles for CNN design.

** AnyNet Design Space
- Networks consist of a stem, body, and head.
- **Stem**: Initial image processing, often with larger window convolutions.
- **Body**: Multiple stages, each reducing spatial resolution.
- **Head**: Converts features to outputs (e.g., softmax for classification).

*** AnyNet Structure
- Stem: 3x3 convolution with stride 2, batch norm, ReLU.
- Body: 4 stages, each with ResNeXt blocks.
- Head: Global average pooling followed by a fully connected layer.

*** ResNeXt Block
- Uses grouped convolutions.
- First block in each stage halves resolution with stride 2.
- Subsequent blocks maintain resolution and channels.

*** Design Parameters
- Block width (number of channels): \(c_0, c_1, c_2, c_3, c_4\).
- Depth (number of blocks per stage): \(d_1, d_2, d_3, d_4\).
- Bottleneck ratios: \(k_1, k_2, k_3, k_4\).
- Group widths (number of groups): \(g_1, g_2, g_3, g_4\).

** Distributions and Parameters of Design Spaces
- Goal: Identify good parameters for the AnyNet design space.
- Challenges: Large number of configurations (e.g., 17 parameters with 2 choices each lead to 131,072 combinations).
- Strategy: Use assumptions to reduce the design space.

*** Assumptions
1. General design principles exist, so many networks satisfying these principles should perform well.
2. Intermediate results can be used as proxies for final accuracy (multi-fidelity optimization).
3. Results from smaller networks generalize to larger ones.
4. Design aspects can be factorized, allowing independent optimization.

*** Empirical CDF
- \(F(e, p)\): Cumulative distribution function for errors committed by networks drawn from distribution \(p\).
- Empirical CDF: \(\hat{F}(e, Z) = \frac{1}{n} \sum_{i=1}^n 1(e_i \leq e)\), where \(Z = \{net_1, ..., net_n\}\).

*** Simplifying Design Space
- Shared bottleneck ratio \(k_i = k\) for all stages.
- Shared group width \(g_i = g\) for all stages.
- Increasing channels and depth across stages improves performance.

** Code Implementation
- AnyNet class in PyTorch:
  - Stem: 3x3 convolution, batch norm, ReLU.
  - Stage: ResNeXt blocks with optional downsampling.
  - Head: Global average pooling, flatten, fully connected layer.

#+BEGIN_SRC python
class AnyNet(d2l.Classifier):
    def stem(self, num_channels):
        return nn.Sequential(
            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),
            nn.LazyBatchNorm2d(), nn.ReLU())

    def stage(self, depth, num_channels, groups, bot_mul):
        blk = []
        for i in range(depth):
            if i == 0:
                blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,
                                            use_1x1conv=True, strides=2))
            else:
                blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))
        return nn.Sequential(*blk)

    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):
        super(AnyNet, self).__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(self.stem(stem_channels))
        for i, s in enumerate(arch):
            self.net.add_module(f'stage{i+1}', self.stage(*s))
        self.net.add_module('head', nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.LazyLinear(num_classes)))
        self.net.apply(d2l.init_cnn)
#+END_SRC
